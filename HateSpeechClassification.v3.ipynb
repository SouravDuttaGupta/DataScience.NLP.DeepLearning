{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f872844f178d90776dc1b06d25cd6b819222199"
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "#### Hate Speech Classification\n",
    "\n",
    "Hate speech is an unfortunately common occurrence on the Internet. Often social media sites like Facebook and Twitter face the problem of identifying and censoring problematic posts while weighing the right to freedom of speech. The importance of detecting and moderating hate speech is evident from the strong connection between hate speech and actual hate crimes. Early identification of users promoting hate speech could enable outreach programs that attempt to prevent an escalation from speech to action.\n",
    "\n",
    "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
    "\n",
    "Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.\n",
    "\n",
    "#### Data Source \n",
    "https://datahack.analyticsvidhya.com/contest/hate-speech-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import os\n",
    "import gc\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense,Dropout,Embedding,LSTM, CuDNNGRU, Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,GlobalMaxPool1D,SpatialDropout1D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load dataset\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test_tweets.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning\n",
    "\n",
    "- punctuation removal\n",
    "- stopwords removal\n",
    "- lemitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "stopword = stopwords.words(\"english\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # punctuation removal\n",
    "    text = \"\".join(p for p in text if p not in punctuation)\n",
    "    \n",
    "    # stopwords removal\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopword]\n",
    "    \n",
    "    # lemitization\n",
    "    words = [lem.lemmatize(word,'v') for word in words]\n",
    "    words = [lem.lemmatize(word,'n') for word in words]\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned'] = train['tweet'].apply(clean)\n",
    "test['cleaned'] = test['tweet'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Working with train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 (a). Apply to_categorical on label variable\n",
    "Converts a class vector (integers) to binary class matrix.\n",
    "np.utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes-1) to one-hot vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = to_categorical(train['label'])\n",
    "train = train.drop('label', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 (b). Split train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train['cleaned'], target, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 (c). Tokenize words\n",
    "A sentence or data can be split into words using the method word_tokenize():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \" \".join(x_train)\n",
    "words = nltk.word_tokenize(words)\n",
    "dist = nltk.FreqDist(words)\n",
    "num_unique_words = len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_len = []\n",
    "for w in x_train:\n",
    "    word=nltk.word_tokenize(w)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "max_len = np.max(r_len)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = num_unique_words\n",
    "max_words = max_len\n",
    "batch_size = 128\n",
    "embed_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Text Preprocessing using Keras Tokenizer\n",
    "- This class allows to vectorize a text corpus, by turning each text into either a sequence of integers or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "x_test  = tokenizer.texts_to_sequences(test['cleaned'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sequence Preprocessing using keras.preprocessing.sequence.pad_sequences\n",
    "- Pads sequences to the same length. Sequences longer than num_timesteps are truncated so that they fit the desired length. The position where padding or truncation happens is determined by the arguments padding and truncating, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "#print(x_train.shape)\n",
    "#print(x_val.shape)\n",
    "#print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Initiate a Model\n",
    "Example:\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "a = Input(shape=(32,))\n",
    "b = Dense(32)(a)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "\n",
    "~ This model will include all layers required in the computation of b given a.\n",
    "~ Compile : \n",
    "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 18:37:16.016825  2372 deprecation_wrapper.py:119] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0820 18:37:16.034679  2372 deprecation_wrapper.py:119] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0820 18:37:16.037159  2372 deprecation_wrapper.py:119] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0820 18:37:16.333256  2372 deprecation_wrapper.py:119] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0820 18:37:16.337719  2372 deprecation.py:506] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0820 18:37:16.357559  2372 deprecation_wrapper.py:119] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0820 18:37:16.384839  2372 deprecation_wrapper.py:119] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 26, 300)           11084100  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 26, 128)           140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 11,226,358\n",
      "Trainable params: 11,226,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_words,))\n",
    "x = Embedding(max_features, embed_dim)(inp)\n",
    "x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(2, activation = \"softmax\")(x)\n",
    "\n",
    "model1 = Model(inputs = inp, outputs=x)\n",
    "model1.compile(loss = 'categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Fit a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 18:37:24.272287  2372 deprecation.py:323] From C:\\Users\\Sourav\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/20\n",
      "25569/25569 [==============================] - 24s 926us/step - loss: 0.3232 - acc: 0.9260 - val_loss: 0.2185 - val_acc: 0.9323\n",
      "Epoch 2/20\n",
      "25569/25569 [==============================] - 23s 898us/step - loss: 0.1467 - acc: 0.9477 - val_loss: 0.1201 - val_acc: 0.9590\n",
      "Epoch 3/20\n",
      "25569/25569 [==============================] - 23s 887us/step - loss: 0.0535 - acc: 0.9816 - val_loss: 0.1233 - val_acc: 0.9603\n",
      "Epoch 4/20\n",
      "25569/25569 [==============================] - 22s 858us/step - loss: 0.0225 - acc: 0.9930 - val_loss: 0.1602 - val_acc: 0.9626\n",
      "Epoch 5/20\n",
      "25569/25569 [==============================] - 23s 896us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.1796 - val_acc: 0.9585\n",
      "Epoch 6/20\n",
      "25569/25569 [==============================] - 23s 883us/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.2066 - val_acc: 0.9609\n",
      "Epoch 7/20\n",
      "25569/25569 [==============================] - 23s 903us/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.2274 - val_acc: 0.9603\n",
      "Epoch 8/20\n",
      "25569/25569 [==============================] - 22s 854us/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.2358 - val_acc: 0.9590\n",
      "Epoch 9/20\n",
      "25569/25569 [==============================] - 23s 908us/step - loss: 0.0028 - acc: 0.9986 - val_loss: 0.2474 - val_acc: 0.9581\n",
      "Epoch 10/20\n",
      "25569/25569 [==============================] - 24s 930us/step - loss: 0.0019 - acc: 0.9989 - val_loss: 0.2672 - val_acc: 0.9589\n",
      "Epoch 11/20\n",
      "25569/25569 [==============================] - 25s 964us/step - loss: 0.0019 - acc: 0.9989 - val_loss: 0.2714 - val_acc: 0.9595\n",
      "Epoch 12/20\n",
      "25569/25569 [==============================] - 24s 921us/step - loss: 0.0019 - acc: 0.9986 - val_loss: 0.2685 - val_acc: 0.9611\n",
      "Epoch 13/20\n",
      "25569/25569 [==============================] - 25s 987us/step - loss: 0.0018 - acc: 0.9990 - val_loss: 0.2760 - val_acc: 0.9606\n",
      "Epoch 14/20\n",
      "25569/25569 [==============================] - 23s 905us/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.2890 - val_acc: 0.9601\n",
      "Epoch 15/20\n",
      "25569/25569 [==============================] - 22s 851us/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.2970 - val_acc: 0.9587\n",
      "Epoch 16/20\n",
      "25569/25569 [==============================] - 22s 878us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.3005 - val_acc: 0.9587\n",
      "Epoch 17/20\n",
      "25569/25569 [==============================] - 22s 855us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.2981 - val_acc: 0.9582\n",
      "Epoch 18/20\n",
      "25569/25569 [==============================] - 24s 938us/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.3085 - val_acc: 0.9592\n",
      "Epoch 19/20\n",
      "25569/25569 [==============================] - 26s 1ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.3035 - val_acc: 0.9592\n",
      "Epoch 20/20\n",
      "25569/25569 [==============================] - 24s 945us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.3081 - val_acc: 0.9568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21e5c0b2438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x_train, y_train, batch_size=512, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97691149 0.66826923]\n"
     ]
    }
   ],
   "source": [
    "pred1 = np.round(np.clip(model1.predict(x_val),  0, 1))\n",
    "print(f1_score(y_val, pred1, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Predict target variable for test and save the result in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=np.round(np.clip(model1.predict(x_test), 0, 1)).astype(int)\n",
    "pred1 = pd.DataFrame(pred1)\n",
    "pred1 = pred1.idxmax(axis=1)\n",
    "submission_GRU = pd.DataFrame({'id':test['id'], 'label':pred1})\n",
    "submission_GRU.to_csv(\"submission_GRU.v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
